{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5989f8fa-30bd-4d35-8894-9ee2f8e29f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.11/site-packages (3.6.7)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: pyldavis in /opt/anaconda3/lib/python3.11/site-packages (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.24.2 in /opt/anaconda3/lib/python3.11/site-packages (from pyldavis) (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.11/site-packages (from pyldavis) (1.12.0)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from pyldavis) (2.2.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from pyldavis) (1.2.0)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from pyldavis) (3.1.3)\n",
      "Requirement already satisfied: numexpr in /opt/anaconda3/lib/python3.11/site-packages (from pyldavis) (2.8.7)\n",
      "Requirement already satisfied: funcy in /opt/anaconda3/lib/python3.11/site-packages (from pyldavis) (2.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from pyldavis) (1.3.0)\n",
      "Requirement already satisfied: gensim in /opt/anaconda3/lib/python3.11/site-packages (from pyldavis) (4.3.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from pyldavis) (68.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas>=2.0.0->pyldavis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas>=2.0.0->pyldavis) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.11/site-packages (from pandas>=2.0.0->pyldavis) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.0.0->pyldavis) (2.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.11/site-packages (from gensim->pyldavis) (5.2.1)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from gensim->pyldavis) (2.0.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->pyldavis) (2.1.3)\n",
      "Requirement already satisfied: pyfume in /opt/anaconda3/lib/python3.11/site-packages (from FuzzyTM>=0.4.0->gensim->pyldavis) (0.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyldavis) (1.16.0)\n",
      "Requirement already satisfied: simpful in /opt/anaconda3/lib/python3.11/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim->pyldavis) (2.12.0)\n",
      "Requirement already satisfied: fst-pso in /opt/anaconda3/lib/python3.11/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim->pyldavis) (1.8.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.11/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim->pyldavis) (4.9.0)\n",
      "Requirement already satisfied: miniful in /opt/anaconda3/lib/python3.11/site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim->pyldavis) (0.0.6)\n",
      "Requirement already satisfied: langdetect in /opt/anaconda3/lib/python3.11/site-packages (1.0.9)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.11/site-packages (from langdetect) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "#Load Libraries\n",
    "!pip install nltk\n",
    "!pip install pyldavis\n",
    "!pip install langdetect\n",
    "import pandas as pd \n",
    "import gensim\n",
    "from gensim import corpora,models \n",
    "from gensim.models import Phrases\n",
    "from gensim.models import LdaModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1f3ad0f9-7ac0-48cc-a509-6195c0c72ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>If everyone thinks you're worthless, then mayb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>Hello, and thank you for your question and see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>First thing I'd suggest is getting the sleep y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>Therapy is essential for those that are feelin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>I first want to let you know that you are not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3507</th>\n",
       "      <td>My grandson's step-mother sends him to school ...</td>\n",
       "      <td>Absolutely not! It is never in a child's best ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3508</th>\n",
       "      <td>My boyfriend is in recovery from drug addictio...</td>\n",
       "      <td>I'm sorry you have tension between you and you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3509</th>\n",
       "      <td>The birth mother attempted suicide several tim...</td>\n",
       "      <td>The true answer is, \"no one can really say wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3510</th>\n",
       "      <td>I think adult life is making him depressed and...</td>\n",
       "      <td>How do you help yourself to believe you requir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3511</th>\n",
       "      <td>I just took a job that requires me to travel f...</td>\n",
       "      <td>hmm this is a tough one!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3512 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Context  \\\n",
       "0     I'm going through some things with my feelings...   \n",
       "1     I'm going through some things with my feelings...   \n",
       "2     I'm going through some things with my feelings...   \n",
       "3     I'm going through some things with my feelings...   \n",
       "4     I'm going through some things with my feelings...   \n",
       "...                                                 ...   \n",
       "3507  My grandson's step-mother sends him to school ...   \n",
       "3508  My boyfriend is in recovery from drug addictio...   \n",
       "3509  The birth mother attempted suicide several tim...   \n",
       "3510  I think adult life is making him depressed and...   \n",
       "3511  I just took a job that requires me to travel f...   \n",
       "\n",
       "                                               Response  \n",
       "0     If everyone thinks you're worthless, then mayb...  \n",
       "1     Hello, and thank you for your question and see...  \n",
       "2     First thing I'd suggest is getting the sleep y...  \n",
       "3     Therapy is essential for those that are feelin...  \n",
       "4     I first want to let you know that you are not ...  \n",
       "...                                                 ...  \n",
       "3507  Absolutely not! It is never in a child's best ...  \n",
       "3508  I'm sorry you have tension between you and you...  \n",
       "3509  The true answer is, \"no one can really say wit...  \n",
       "3510  How do you help yourself to believe you requir...  \n",
       "3511                           hmm this is a tough one!  \n",
       "\n",
       "[3512 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data\n",
    "# use read_csv to read csv file, not read_table\n",
    "df = pd.read_csv('train.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bf4588f0-5c80-422f-abe1-304b854353a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, and thank you for your question and seeking advice on this. Feelings of worthlessness is unfortunately common. In fact, most people, if not all, have felt this to some degree at some point in their life. You are not alone.\\xa0Changing our feelings is like changing our thoughts - it\\'s hard to do. Our minds are so amazing that the minute you change your thought another one can be right there to take it\\'s place. Without your permission, another thought can just pop in there. The new thought may feel worse than the last one! My guess is that you have tried several things to improve this on your own even before reaching out on here. People often try thinking positive thoughts, debating with their thoughts, or simply telling themselves that they need to \"snap out of it\" - which is also a thought that carries some self-criticism.\\xa0Some people try a different approach, and there are counselors out there that can help you with this. The idea is that instead of trying to change the thoughts, you change how you respond to them. You learn skills that allow you to manage difficult thoughts and feelings differently so they don\\'t have the same impact on you that they do right now. For some people, they actually DO begin to experience less hurtful thoughts once they learn how to manage the ones they have differently. Acceptance and Commitment Therapy may be a good choice for you.\\xa0There is information online and even self-help books that you can use to teach you the skills that I mentioned. Because they are skills, they require practice, but many people have found great relief and an enriched life by learning them.\\xa0As for suicidal thoughts, I am very glad to read that this has not happened to you. Still, you should watch out for this because it can be a sign of a worsening depression. If you begin to think about this, it is important to reach out to a support system right away. The National Suicide Prevention Lifeline is 1-800-273-8255. The text line is #741741.\\xa0I hope some other colleagues will provide you more suggestions.\\xa0Be well...Robin Landwehr, DBH, LPCC'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert all review text into list format\n",
    "docs = df['Response'].tolist()\n",
    "docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c003383-c4ea-4185-9c6d-7dd9e58d676d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'and', 'thank', 'you', 'for', 'your', 'question', 'and', 'seeking', 'advice', 'on', 'this', 'feelings', 'of', 'worthlessness', 'is', 'unfortunately', 'common', 'in', 'fact', 'most', 'people', 'if', 'not', 'all', 'have', 'felt', 'this', 'to', 'some', 'degree', 'at', 'some', 'point', 'in', 'their', 'life', 'you', 'are', 'not', 'alone', 'changing', 'our', 'feelings', 'is', 'like', 'changing', 'our', 'thoughts', 'it', 's', 'hard', 'to', 'do', 'our', 'minds', 'are', 'so', 'amazing', 'that', 'the', 'minute', 'you', 'change', 'your', 'thought', 'another', 'one', 'can', 'be', 'right', 'there', 'to', 'take', 'it', 's', 'place', 'without', 'your', 'permission', 'another', 'thought', 'can', 'just', 'pop', 'in', 'there', 'the', 'new', 'thought', 'may', 'feel', 'worse', 'than', 'the', 'last', 'one', 'my', 'guess', 'is', 'that', 'you', 'have', 'tried', 'several', 'things', 'to', 'improve', 'this', 'on', 'your', 'own', 'even', 'before', 'reaching', 'out', 'on', 'here', 'people', 'often', 'try', 'thinking', 'positive', 'thoughts', 'debating', 'with', 'their', 'thoughts', 'or', 'simply', 'telling', 'themselves', 'that', 'they', 'need', 'to', 'snap', 'out', 'of', 'it', 'which', 'is', 'also', 'a', 'thought', 'that', 'carries', 'some', 'self', 'criticism']\n"
     ]
    }
   ],
   "source": [
    "# Assuming you've loaded your CSV file into a DataFrame `df`\n",
    "docs = df['Response'].astype(str)  # Replace 'text_column_name' with the name of your column\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Now 'docs' is a Series object where each document is a string\n",
    "# We'll apply the tokenizer to each document in the Series.\n",
    "docs = docs.str.lower().apply(tokenizer.tokenize)\n",
    "\n",
    "# Let's take a look at the first 150 tokens of the second document\n",
    "# Ensure that there are enough tokens for the second document\n",
    "if len(docs) > 1 and len(docs[1]) > 150:\n",
    "    print(docs[1][:150])\n",
    "else:\n",
    "    print(\"Document 1 does not have 150 tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "783214ce-d50b-4688-8e75-f1849f6dd49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'and', 'thank', 'you', 'for', 'your', 'question', 'and', 'seeking', 'advice', 'on', 'this', 'feelings', 'of', 'worthlessness', 'is', 'unfortunately', 'common', 'in', 'fact', 'most', 'people', 'if', 'not', 'all', 'have', 'felt', 'this', 'to', 'some', 'degree', 'at', 'some', 'point', 'in', 'their', 'life', 'you', 'are', 'not', 'alone', 'changing', 'our', 'feelings', 'is', 'like', 'changing', 'our', 'thoughts', 'it']\n"
     ]
    }
   ],
   "source": [
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "    \n",
    "print(docs[1][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c57dfe5f-f70c-4b26-8ff0-488b32554019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'thank', 'question', 'seeking', 'advice', 'feelings', 'worthlessness', 'unfortunately', 'common', 'fact', 'people', 'felt', 'degree', 'point', 'life', 'alone', 'changing', 'feelings', 'like', 'changing', 'thoughts', 'hard', 'minds', 'amazing', 'minute', 'change', 'thought', 'another', 'one', 'right', 'take', 'place', 'without', 'permission', 'another', 'thought', 'pop', 'new', 'thought', 'may', 'feel', 'worse', 'last', 'one', 'guess', 'tried', 'several', 'things', 'improve', 'even']\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords.\n",
    "docs = [[token for token in doc if token not in stopwords.words('english')] for doc in docs]\n",
    "print(docs[1][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "636b4fde-4407-4961-921a-ff217ed3a831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'thank', 'question', 'seeking', 'advice', 'feelings', 'worthlessness', 'unfortunately', 'common', 'fact', 'people', 'felt', 'degree', 'point', 'life', 'alone', 'changing', 'feelings', 'like', 'changing', 'thoughts', 'hard', 'minds', 'amazing', 'minute', 'change', 'thought', 'another', 'one', 'right', 'take', 'place', 'without', 'permission', 'another', 'thought', 'pop', 'new', 'thought', 'may', 'worse', 'last', 'one', 'guess', 'tried', 'several', 'things', 'improve', 'even', 'reaching']\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords including fashion-specific ones\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "therapy_specific_stopwords = {'u','yes','okay','sure','really','very','quite','is','are','have','be','feel','think','talk','therapy','session','patient','client'}\n",
    "all_stopwords = english_stopwords.union(therapy_specific_stopwords)\n",
    "docs = [[token for token in doc if token not in all_stopwords] for doc in docs]\n",
    "print(docs[1][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "08ca00f2-a394-4b95-aea9-8b6a566b0184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'thank', 'question', 'seeking', 'advice', 'feelings', 'worthlessness', 'unfortunately', 'common', 'fact', 'people', 'felt', 'degree', 'point', 'life', 'alone', 'changing', 'feelings', 'like', 'changing', 'thoughts', 'hard', 'minds', 'amazing', 'minute', 'change', 'thought', 'another', 'one', 'right', 'take', 'place', 'without', 'permission', 'another', 'thought', 'pop', 'new', 'thought', 'may', 'worse', 'last', 'one', 'guess', 'tried', 'several', 'things', 'improve', 'even', 'reaching']\n"
     ]
    }
   ],
   "source": [
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]\n",
    "print(docs[1][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5375fa6a-ef4a-4613-a5fe-b088e31db455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'thank', 'question', 'seeking', 'advice', 'feeling', 'worthlessness', 'unfortunately', 'common', 'fact', 'people', 'felt', 'degree', 'point', 'life', 'alone', 'changing', 'feeling', 'like', 'changing', 'thought', 'hard', 'mind', 'amazing', 'minute', 'change', 'thought', 'another', 'one', 'right', 'take', 'place', 'without', 'permission', 'another', 'thought', 'pop', 'new', 'thought', 'may', 'worse', 'last', 'one', 'guess', 'tried', 'several', 'thing', 'improve', 'even', 'reaching']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize the documents.\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "print(docs[1][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4ff4b730-b93b-461f-8388-687f0a53d426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['everyone', 'think', 'worthless', 'maybe', 'need', 'find', 'new', 'people', 'hang', 'seriously', 'social', 'context', 'person', 'life', 'big', 'influence', 'self_esteem', 'otherwise', 'go', 'round', 'round', 'trying', 'understand', 'worthless', 'go', 'back', 'crowd', 'knocked', 'many', 'inspirational', 'message', 'find', 'social_medium', 'maybe', 'read', 'one', 'state', 'person', 'worthless', 'everyone', 'good', 'purpose', 'life', 'also', 'since', 'culture', 'saturated', 'belief', 'someone', 'good', 'somehow', 'terrible', 'bad', 'feeling', 'part', 'living', 'motivation', 'remove', 'situation', 'relationship', 'u', 'harm', 'good', 'bad', 'feeling', 'terrible', 'feeling', 'worthlessness', 'may', 'good', 'sense', 'motivating', 'find', 'much', 'better', 'feeling', 'today']\n"
     ]
    }
   ],
   "source": [
    "# use Phrases to Compute bigrams.\n",
    "\n",
    "\n",
    "# Add bigrams to docs (only ones that appear 10 times or more).\n",
    "bigram = Phrases(docs,min_count = 5, threshold=40)\n",
    "\n",
    "\n",
    "print(bigram[docs[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82eb0ca-8b0f-43ae-871d-8c1e10ffbff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Let's assume 'docs' is your list of preprocessed documents (each document is a list of words)\n",
    "dictionary = corpora.Dictionary(docs)  # Creates a dictionary of all unique words\n",
    "\n",
    "# Convert document into the bag-of-words format\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "# Apply TF-IDF\n",
    "tfidf = models.TfidfModel(corpus)  # Initialize a model\n",
    "corpus_tfidf = tfidf[corpus]  # Apply transformation to the entire corpus\n",
    "\n",
    "# Running LDA using Bag of Words\n",
    "lda_model_bow = LdaModel(corpus, num_topics=10, id2word=dictionary, passes=15, random_state=100)\n",
    "\n",
    "# Running LDA using TF-IDF\n",
    "lda_model_tfidf = LdaModel(corpus_tfidf, num_topics=10, id2word=dictionary, passes=15, random_state=100)\n",
    "\n",
    "# Save the model to disk if needed\n",
    "lda_model_tfidf.save('model_tfidf.lda')\n",
    "\n",
    "# View the topics\n",
    "print(lda_model_tfidf.print_topics())\n",
    "\n",
    "# Assuming 'lda_model_tfidf' and 'corpus_tfidf' are already defined along with 'dictionary'\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model_tfidf, corpus_tfidf, dictionary)\n",
    "pyLDAvis.display(vis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2efaa952-d0bc-4cef-9500-f64d156a0b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis, 'lda_topic_modeling.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45f0e0-0e01-46b2-809d-fb5dc328e478",
   "metadata": {},
   "source": [
    "### Topic Modeling Visualization (Static Image)\n",
    "\n",
    "The topic modeling results below were originally created with pyLDAvis as an interactive chart.  \n",
    "Since GitHub does not support interactive visualizations in notebooks, a static version is shown here.\n",
    "\n",
    "![LDA Topic Modeling](lda_topic_modeling.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f73f99d6-5ff9-4a90-9577-426ad4d524e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.002*\"termination\" + 0.002*\"refer\" + 0.001*\"improving\" + 0.001*\"transgender\" + 0.001*\"polyamorous\" + 0.001*\"goodbye\" + 0.001*\"seasonal\" + 0.001*\"terminate\" + 0.001*\"essentially\" + 0.001*\"shared\"\n",
      "Topic: 1 \n",
      "Words: 0.003*\"diagnosis\" + 0.003*\"consultation\" + 0.002*\"bother\" + 0.002*\"faith\" + 0.002*\"courage\" + 0.002*\"worthless\" + 0.001*\"expertise\" + 0.001*\"domestic\" + 0.001*\"gut\" + 0.001*\"increased\"\n",
      "Topic: 2 \n",
      "Words: 0.011*\"que\" + 0.010*\"de\" + 0.008*\"la\" + 0.005*\"en\" + 0.004*\"el\" + 0.004*\"tu\" + 0.003*\"e\" + 0.003*\"para\" + 0.003*\"con\" + 0.003*\"te\"\n",
      "Topic: 3 \n",
      "Words: 0.002*\"dog\" + 0.001*\"seizure\" + 0.001*\"depressive\" + 0.001*\"breakup\" + 0.001*\"id\" + 0.001*\"craving\" + 0.001*\"card\" + 0.001*\"deepest\" + 0.001*\"affordable\" + 0.001*\"psychopath\"\n",
      "Topic: 4 \n",
      "Words: 0.002*\"certainly\" + 0.002*\"weekend\" + 0.001*\"brave\" + 0.001*\"apprehensive\" + 0.001*\"evaluation\" + 0.001*\"cheat\" + 0.001*\"wedding\" + 0.001*\"alliance\" + 0.001*\"extremely\" + 0.001*\"military\"\n",
      "Topic: 5 \n",
      "Words: 0.002*\"helpfulness\" + 0.002*\"nugget\" + 0.002*\"four\" + 0.002*\"blog\" + 0.002*\"dream\" + 0.002*\"bf\" + 0.002*\"liking\" + 0.002*\"check\" + 0.002*\"add\" + 0.002*\"listi\"\n",
      "Topic: 6 \n",
      "Words: 0.004*\"nan\" + 0.001*\"imposter\" + 0.001*\"syndrome\" + 0.001*\"dream\" + 0.001*\"approval\" + 0.001*\"anti\" + 0.001*\"depressant\" + 0.001*\"stream\" + 0.001*\"courage\" + 0.001*\"irrational\"\n",
      "Topic: 7 \n",
      "Words: 0.001*\"obsession\" + 0.001*\"no\" + 0.001*\"coverage\" + 0.001*\"postpartum\" + 0.001*\"empty\" + 0.001*\"profound\" + 0.001*\"bully\" + 0.001*\"coraje\" + 0.001*\"dysphoria\" + 0.001*\"positivity\"\n",
      "Topic: 8 \n",
      "Words: 0.003*\"dream\" + 0.002*\"animal\" + 0.002*\"bed\" + 0.001*\"excitement\" + 0.001*\"performance\" + 0.001*\"summer\" + 0.001*\"rely\" + 0.001*\"negatively\" + 0.001*\"baby\" + 0.001*\"pedophile\"\n",
      "Topic: 9 \n",
      "Words: 0.004*\"relationship\" + 0.003*\"therapist\" + 0.003*\"feeling\" + 0.003*\"may\" + 0.003*\"would\" + 0.003*\"help\" + 0.003*\"time\" + 0.003*\"want\" + 0.003*\"thing\" + 0.003*\"people\"\n"
     ]
    }
   ],
   "source": [
    "# Let's assume 'docs' is your list of preprocessed documents (each document is a list of words)\n",
    "dictionary = corpora.Dictionary(docs)  # This creates a dictionary of all unique words\n",
    "\n",
    "# Convert document into the bag-of-words format\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "# Apply TF-IDF\n",
    "tfidf = models.TfidfModel(corpus)  # Initialize a model\n",
    "corpus_tfidf = tfidf[corpus]  # Apply transformation to the entire corpus\n",
    "\n",
    "# Running LDA using TF-IDF\n",
    "lda_model_tfidf = LdaModel(corpus_tfidf, num_topics=10, id2word=dictionary, passes=15, random_state=100)\n",
    "\n",
    "# To view the top 10 words for each topic\n",
    "for i, topic in lda_model_tfidf.print_topics(-1, num_words=10):\n",
    "    print('Topic: {} \\nWords: {}'.format(i, topic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "47ed71e5-fb55-4974-a5db-a2e6d65aaa79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Topic 1</td>\n",
       "      <td>termination, refer, improving, transgender, po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Topic 2</td>\n",
       "      <td>diagnosis, consultation, bother, faith, courag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Topic 3</td>\n",
       "      <td>que, de, la, en, el, tu, e, para, con, te</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Topic 4</td>\n",
       "      <td>dog, seizure, depressive, breakup, id, craving...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Topic 5</td>\n",
       "      <td>certainly, weekend, brave, apprehensive, evalu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Topic 6</td>\n",
       "      <td>helpfulness, nugget, four, blog, dream, bf, li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Topic 7</td>\n",
       "      <td>nan, imposter, syndrome, dream, approval, anti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Topic 8</td>\n",
       "      <td>obsession, no, coverage, postpartum, empty, pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Topic 9</td>\n",
       "      <td>dream, animal, bed, excitement, performance, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Topic 10</td>\n",
       "      <td>relationship, therapist, feeling, may, would, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Topic                                              Words\n",
       "0   Topic 1  termination, refer, improving, transgender, po...\n",
       "1   Topic 2  diagnosis, consultation, bother, faith, courag...\n",
       "2   Topic 3          que, de, la, en, el, tu, e, para, con, te\n",
       "3   Topic 4  dog, seizure, depressive, breakup, id, craving...\n",
       "4   Topic 5  certainly, weekend, brave, apprehensive, evalu...\n",
       "5   Topic 6  helpfulness, nugget, four, blog, dream, bf, li...\n",
       "6   Topic 7  nan, imposter, syndrome, dream, approval, anti...\n",
       "7   Topic 8  obsession, no, coverage, postpartum, empty, pr...\n",
       "8   Topic 9  dream, animal, bed, excitement, performance, s...\n",
       "9  Topic 10  relationship, therapist, feeling, may, would, ..."
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assume lda_model_tfidf is your trained LDA model\n",
    "top_words_per_topic = []\n",
    "\n",
    "for t in range(lda_model_tfidf.num_topics):\n",
    "    top_words = lda_model_tfidf.show_topic(t, 10)\n",
    "    topic_words = \", \".join([word for word, prob in top_words])\n",
    "    top_words_per_topic.append((f\"Topic {t+1}\", topic_words))\n",
    "\n",
    "# Create a DataFrame\n",
    "df_top_words = pd.DataFrame(top_words_per_topic, columns=['Topic', 'Words'])\n",
    "\n",
    "# Display the DataFrame\n",
    "df_top_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629f4e12-a6dd-44ae-a775-7b99cb3df69a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
